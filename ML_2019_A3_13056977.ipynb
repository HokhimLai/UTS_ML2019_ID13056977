{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_2019_A3_13056977.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HokhimLai/UTS_ML2019_ID13056977/blob/master/ML_2019_A3_13056977.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9en1QAypTGR_",
        "colab_type": "text"
      },
      "source": [
        "`Assignment 3: Take Home Exam Question2: Ensemblem Method`\n",
        "\n",
        "`Student ID: 13056977`\n",
        "\n",
        "`Student Name: HokHim Lai`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13b4DNkLLyMz",
        "colab_type": "text"
      },
      "source": [
        "#0 Question \n",
        "\n",
        "QUESTION 2\n",
        "Ensemble methods have been very successful in building classifiers. The hot topics\n",
        "include how to create diverse classifiers and how to fuse the decisions from\n",
        "individual classifiers, in particular how to establish the weights that individual\n",
        "classifiers contribute to the ensemble’s answer. Describe two existing approaches\n",
        "to solving this problem, discuss their advantages and disadvantages. Make a plan\n",
        "to address one issue or two (related to learning the weights or creating diverse\n",
        "classifiers), briefly describe your new method. Explain the reason why the\n",
        "developed method could outperform the conventional ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfnXVWSDbhWf",
        "colab_type": "text"
      },
      "source": [
        "# 1 Introduction\n",
        "\n",
        "Ensemble method is a famous and popular method have been genuinely applied into machine learning  field. This method was proposed in the early 1980 in the Dasarathy and Sheela’s paper. At that time, the idea was very rough and simple, they only use applied partitioning feature with two or more classifiers. Over a decade, many researchers had been heavily developed into this area and today can turn to more than 7 kinds of methods to simulate this methodology. \n",
        "\n",
        "The idea of ensemble learning is using multiple learning algorithms at the same time. In usual cases, it would only apply an individual classifier or method to analyse one dataset and then compare the model accuracy with other learning algorithms. However, ensemble learning is introduced by analysing a dataset with more than one algorithm at the same time. By adding the weight or the average of the accacry to balance the model and finally come up with a high predict rate model. Today, there are already a lot of types of  ensemble learning have been used. For example, Bagging, Boosting, Bayes optimal classifier and stacking. These examples are just commonly model are used today, there are still many other kinds of ensemble learning algorithm in machine learning industry. In this project, we will focusly talk two common approaches, they are Bagging and Boosting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXrMC15el6Z4",
        "colab_type": "text"
      },
      "source": [
        "# 2 Bagging\n",
        "\n",
        "Bagging is one of the most common methods that have been used among  ensemble learning algorithm, it can also be stands for bootstrap aggregation. Bagging is  a technique for learning many classifiers and each only using only portions  of the data and then combining them through a model averaging technique. The idea behind this is to reduce overfitting of a class of models. When we overfit, we would memorize the data set and we would get a far lower training error on that training set, then we would see on say a new validation or test data, cross-validation is used when this situation happened. The idea is to try to test or check to see whether we had overfit. The cross-validation will take our dataset and we would repeatedly split up into one part which would train a model with a given complexity level and another part would check to see whether we had overfit. The idea of bagging is to do a similar kind of data splitting or resampling technique but instead of using them to check to see whether we overfit, we instead try to combine them so that we produce a better classifier or predictor.\n",
        "\n",
        "\n",
        "\n",
        "Bagging relies on a classical statistical technique called bootstrap which creates a random new subset of data by sampling by the given dataset. The idea is we have a particular dataset and we would like to generate a similar dataset by sampling from it with replacement so this is used in statistics to generate confidence values or confidence intervals of estimates and understand what the variation due to the particular realization of the dataset is.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WABghAfsk9Zn",
        "colab_type": "text"
      },
      "source": [
        "##2.1 Advantage \n",
        "\n",
        "The benefit of using bagging ensemble method is it take the average over collection. We take that collection and combine it into an overall average with an unweighted sum, in this case we will take a majority vote so if the majority of classifiers picks a particular  class we would predict that one. Though this process it will reduce the memorization effect since not every predictor can see each data point. Moreover, it can also lower the complexity of the overall average and lead us to have a better generalization performance. Bagging can decrease the variance of every individual model as they combine all the predictions from different models, the result come out can have a have high stability. \n",
        "\n",
        "When we apply bagging in different situations,  it can have better performance compared with other ensemble learning algorithm. If a particular model get a low accuracy and performance, using bagging will rarely get a better bias, as it combines several models at the same time. If a particular model is overfitting, bagging can also have to solve this problem. Because bagging can reduce the complexity of a model class prone to overfit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqms3evYlDVI",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Disadvantage\n",
        "\n",
        "The disadvantage of bagging is, First, for some special dataset, it’s hard to control the data complexity as it is harder for each to memorize the data, since each one is seeing its own individually generated training dataset and that training dataset will sample from the original one so it’s left some point out. And those points won’t be seen by that particular classifier, it won't be able to memorize the full training dataset. Therefore, this doesn’t work on linear models such as linear regression, but perceptrons in bagging is fine. Moreover, for the cost of this method is unseenable.If we learn K bagged predictors, our prediction time and competition becomes K times larger than it would have been before, thus, the cost is unpredictable and based on how many times we simulate the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PBGDPcIlKTU",
        "colab_type": "text"
      },
      "source": [
        "#3 Boosting\n",
        "\n",
        "Boosting is an ensemble techniques similar to bagging. The idea of boosting is to create a sequence of increasingly complex predictors out of building block made out of very simple predictors. Boosting method train model by sequentially training a new simple model based on the errors of the previous model so we start off by learning a very simple predictor and then evaluate its errors and focus the next predictor on getting these example right. These process aims to find out the examples and data points that are hard to predict and focuses later classifier on estimating these examples better at the end. We combine the whole set uses some weighted combination and this a way of scaling up complexity so each method can be very simple and by combining many weak learners that are not able to learn complex functions. After all, we can convert them all into an overall much more complex classifier. \n",
        "\n",
        "Under the boosting method category, there are more than one boost method can be used, for example, AdaBoost, LPBoost or xgboost. In this section we will take AdaBoost as our main model to discus. The step of this model is, first we train our base model algorithm using a normal method. By observing the training data sample that base model predict incorrectly and create weight for these samples. After that we train the second base model and apply the weight to the sample when calculating the model. By repeating this procedure, the model can learn the weak learner from several complex models and finally predict a better result of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK76OdqlPQ9",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Advantage and disadvantage\n",
        "\n",
        "\n",
        "The advantage of boosting is kind of the same as the bagging method. As boosting end up with combine multiple classifiers to make a better classifiers, the commottes is using majority vote and weighted combination. This method can use same or different classifiers. Another advantage is using boosting method can support different loss function, the gradient descent algorithm can optimize any differentiable loss function. Boosting can also generate a combined model with lower errors as it optimise the advantages and reduces pitfalls of single model.\n",
        "\n",
        "However, the disadvantage of this model is Boosting doesn’t not help to avoid overfitting problem. And this is the main issue faced by boosting method either on other dataset. It also requires user careful enough on tuning of different hyper-parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B_O5T7vlhn1",
        "colab_type": "text"
      },
      "source": [
        "# 4 Issue and new methodology "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKTsrJ1FpX8b",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 Issue \n",
        "\n",
        "One of the main issues of ensemble learning is the bias and variance. The balance of bias and variance of the dataset will always lead to problems in the ensemble learning algorithm, such as overfitting. Bias is the average difference between estimated and actual results, high Bias means it will get a poor performance. When We have High bias increase model complexity so we get low bias. Therefore, low bias with low variance will be the best situation for ensemble model to learn. Our new method is aim to reduce the bias and variance in order to let the data can fit into the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2jTSFbDpaiw",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 Methodology"
      ]
    }
  ]
}